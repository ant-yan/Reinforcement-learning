
# Unstable Estimates in Off-Policy Evaluation

This project explores how **ordinary importance sampling** in off-policy evaluation can suffer from **unbounded variance**, using a minimal example from *Sutton & Barto's Reinforcement Learning* (Example 5.5, Figure 5.4).

## Project Summary

Off-policy reinforcement learning enables evaluation of one policy using data generated by another. While **importance sampling** is a key method here, it can produce highly **unstable estimates** in certain environments due to variance that can theoretically become infinite.

This simulation illustrates such a failure case and demonstrates that, despite running many episodes, the estimates do not stabilize.

## Environment Setup

- **Single state** scenario (non-terminal).
- **Action choices**:
  - `right`: ends the episode immediately with reward 0.
  - `left`:
    - 90% chance to return to the same state.
    - 10% chance to end the episode and yield a reward of +1.

### Policy Definitions

- **Target Policy (Ï€):** Always chooses `left`.  
- **Behavior Policy (b):** Chooses `left` and `right` with equal probability (0.5 each).

### Ground Truth

Following the target policy, all episodes eventually end with a reward of +1. Thus, the **true value** of the state is 1.

## Variance Explosion

Because the likelihood ratio used in importance sampling depends on the actions taken, episodes that select `left` repeatedly before terminating accumulate **large importance weights**. This exponential growth causes the **variance to explode**, making convergence practically impossible even over tens of thousands of episodes.

### Visualization

The figure (`figure_5_4.png`) displays how value estimates evolve across 10 separate runs (log-scaled x-axis). The plot shows how the estimates fluctuate wildly and fail to settle, clearly illustrating the variance issue.
