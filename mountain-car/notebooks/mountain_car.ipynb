{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98ec682ac86211d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mountain Car Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610c62bc305fa67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a914be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Figure 1 training: 100%|██████████| 9000/9000 [01:25<00:00, 105.11it/s]\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All figures saved into: /Users/antonstepanyan/Desktop/RL/Reinforcement-Learning/mountain-car/notebooks/generated_pictures\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use a non-interactive backend – we only save figures to files\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Mountain Car environment\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class MCConfig:\n",
    "    min_position: float = -1.2\n",
    "    max_position: float = 0.5\n",
    "    max_speed: float = 0.07\n",
    "    goal_position: float = 0.5\n",
    "    force: float = 0.001\n",
    "    gravity: float = 0.0025\n",
    "    max_steps: int = 300  # same cap used in the book\n",
    "\n",
    "\n",
    "class MountainCar:\n",
    "    \"\"\"\n",
    "    Minimal Mountain Car with three discrete actions:\n",
    "      0 = push left, 1 = coast, 2 = push right\n",
    "    Reward is -1 on every step until the goal is reached.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: MCConfig | None = None) -> None:\n",
    "        self.cfg = cfg or MCConfig()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.position = np.random.uniform(-0.6, -0.4)\n",
    "        self.velocity = 0.0\n",
    "        self.t = 0\n",
    "        return np.array([self.position, self.velocity], dtype=np.float32)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n",
    "        assert action in (0, 1, 2)\n",
    "        cfg = self.cfg\n",
    "\n",
    "        # convert action to acceleration {-1, 0, +1}\n",
    "        accel = action - 1\n",
    "\n",
    "        self.velocity += accel * cfg.force - cfg.gravity * math.cos(3 * self.position)\n",
    "        self.velocity = float(np.clip(self.velocity, -cfg.max_speed, cfg.max_speed))\n",
    "\n",
    "        self.position += self.velocity\n",
    "        self.position = float(np.clip(self.position, cfg.min_position, cfg.max_position))\n",
    "\n",
    "        # stop if we bounce into the left wall\n",
    "        if self.position == cfg.min_position and self.velocity < 0:\n",
    "            self.velocity = 0.0\n",
    "\n",
    "        self.t += 1\n",
    "        done = bool(self.position >= cfg.goal_position or self.t >= cfg.max_steps)\n",
    "        reward = -1.0\n",
    "\n",
    "        state = np.array([self.position, self.velocity], dtype=np.float32)\n",
    "        return state, reward, done\n",
    "\n",
    "\n",
    "# Global environment used in all experiments\n",
    "ENV = MountainCar()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Tile coding\n",
    "# ============================================================\n",
    "class IHT:\n",
    "    \"\"\"\n",
    "    Index Hash Table for tile coding: maps coordinate tuples to\n",
    "    integer indices up to `size`, then falls back to hashing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int) -> None:\n",
    "        self.size = size\n",
    "        self._dict: dict[Tuple[int, ...], int] = {}\n",
    "        self.overfull_count = 0\n",
    "\n",
    "    def get_index(self, key: Tuple[int, ...]) -> int:\n",
    "        if key in self._dict:\n",
    "            return self._dict[key]\n",
    "\n",
    "        if len(self._dict) >= self.size:\n",
    "            if self.overfull_count == 0:\n",
    "                print(\"[IHT] Capacity reached – using hashed indices (collisions possible).\")\n",
    "            self.overfull_count += 1\n",
    "            return hash(key) % self.size\n",
    "\n",
    "        idx = len(self._dict)\n",
    "        self._dict[key] = idx\n",
    "        return idx\n",
    "\n",
    "\n",
    "class TileCoder:\n",
    "    \"\"\"\n",
    "    Tile coder for 2D state (position, velocity) + action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tilings: int,\n",
    "        num_tiles: int,\n",
    "        low: np.ndarray,\n",
    "        high: np.ndarray,\n",
    "        iht_size: int,\n",
    "    ) -> None:\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.low = low.astype(np.float32)\n",
    "        self.high = high.astype(np.float32)\n",
    "        self.iht = IHT(iht_size)\n",
    "\n",
    "        self.tile_width = (self.high - self.low) / (num_tiles - 1)\n",
    "\n",
    "    def _coords_for_tiling(self, tiling: int, state: np.ndarray, action: int) -> Tuple[int, ...]:\n",
    "        # each tiling is slightly shifted\n",
    "        offsets = (tiling / self.num_tilings) * self.tile_width\n",
    "        shifted = state + offsets\n",
    "        ratios = (shifted - self.low) / self.tile_width\n",
    "        indices = np.floor(ratios).astype(int)\n",
    "        return (tiling, int(indices[0]), int(indices[1]), int(action))\n",
    "\n",
    "    def tiles(self, state: np.ndarray, action: int) -> List[int]:\n",
    "        active: List[int] = []\n",
    "        for tiling in range(self.num_tilings):\n",
    "            coords = self._coords_for_tiling(tiling, state, action)\n",
    "            idx = self.iht.get_index(coords)\n",
    "            active.append(idx)\n",
    "        return active\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Action-value function with tile coding\n",
    "# ============================================================\n",
    "class ActionValueFunction:\n",
    "    \"\"\"\n",
    "    Linear approximator for Q(s, a) using tile-coded features:\n",
    "\n",
    "        Q(s, a) = sum_{i in active_tiles(s, a)} w_i\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_size_scaled: float,   # this is α × num_tilings (like in the book)\n",
    "        num_tilings: int = 8,\n",
    "        num_tiles: int = 8,\n",
    "        iht_size: int = 4096,\n",
    "        gamma: float = 1.0,\n",
    "    ) -> None:\n",
    "        self.num_actions = 3\n",
    "        self.num_tilings = num_tilings\n",
    "        self.gamma = gamma\n",
    "\n",
    "        low = np.array([ENV.cfg.min_position, -ENV.cfg.max_speed], dtype=np.float32)\n",
    "        high = np.array([ENV.cfg.max_position, ENV.cfg.max_speed], dtype=np.float32)\n",
    "\n",
    "        self.tile_coder = TileCoder(num_tilings, num_tiles, low, high, iht_size)\n",
    "        self.weights = np.zeros(iht_size, dtype=np.float32)\n",
    "\n",
    "        # real step-size per update\n",
    "        self.alpha = step_size_scaled / num_tilings\n",
    "\n",
    "    def _features(self, state: np.ndarray, action: int) -> List[int]:\n",
    "        return self.tile_coder.tiles(state, action)\n",
    "\n",
    "    def q(self, state: np.ndarray, action: int) -> float:\n",
    "        idxs = self._features(state, action)\n",
    "        return float(self.weights[idxs].sum())\n",
    "\n",
    "    def q_all(self, state: np.ndarray) -> np.ndarray:\n",
    "        return np.array([self.q(state, a) for a in range(self.num_actions)], dtype=np.float32)\n",
    "\n",
    "    def greedy_action(self, state: np.ndarray) -> int:\n",
    "        return int(np.argmax(self.q_all(state)))\n",
    "\n",
    "    def epsilon_greedy(self, state: np.ndarray, epsilon: float) -> int:\n",
    "        if np.random.rand() < epsilon:\n",
    "            return int(np.random.randint(self.num_actions))\n",
    "        return self.greedy_action(state)\n",
    "\n",
    "    def update(self, state: np.ndarray, action: int, target: float) -> None:\n",
    "        idxs = self._features(state, action)\n",
    "        estimate = self.weights[idxs].sum()\n",
    "        delta = target - estimate\n",
    "        self.weights[idxs] += self.alpha * delta\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# One episode of n-step semi-gradient SARSA\n",
    "# ============================================================\n",
    "def run_n_step_sarsa_episode(\n",
    "    value_function: ActionValueFunction,\n",
    "    n: int = 4,\n",
    "    epsilon: float = 0.0,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Run a single n-step semi-gradient SARSA episode.\n",
    "\n",
    "    Returns the number of steps in that episode.\n",
    "    \"\"\"\n",
    "    gamma = value_function.gamma\n",
    "    state = ENV.reset()\n",
    "    action = value_function.epsilon_greedy(state, epsilon)\n",
    "\n",
    "    states = [state]\n",
    "    actions = [action]\n",
    "    rewards = [0.0]  # placeholder for R_0\n",
    "\n",
    "    T = math.inf\n",
    "    t = 0\n",
    "\n",
    "    while True:\n",
    "        if t < T:\n",
    "            next_state, reward, done = ENV.step(actions[t])\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                T = t + 1\n",
    "            else:\n",
    "                next_action = value_function.epsilon_greedy(next_state, epsilon)\n",
    "                actions.append(next_action)\n",
    "\n",
    "        tau = t - n + 1\n",
    "        if tau >= 0:\n",
    "            # n-step return\n",
    "            G = 0.0\n",
    "            for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "\n",
    "            if tau + n < T:\n",
    "                G += (gamma ** n) * value_function.q(states[tau + n], actions[tau + n])\n",
    "\n",
    "            value_function.update(states[tau], actions[tau], G)\n",
    "\n",
    "        if tau == T - 1:\n",
    "            return int(T - 1)  # number of actual steps\n",
    "\n",
    "        t += 1\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Cost-to-go surface\n",
    "# ============================================================\n",
    "def plot_cost_surface(\n",
    "    value_function: ActionValueFunction,\n",
    "    ax,\n",
    "    title: str,\n",
    "    num_positions: int = 50,\n",
    "    num_velocities: int = 50,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Approximate cost-to-go by -max_a Q(s, a) on a grid of states.\n",
    "    \"\"\"\n",
    "    cfg = ENV.cfg\n",
    "    positions = np.linspace(cfg.min_position, cfg.max_position, num_positions)\n",
    "    velocities = np.linspace(-cfg.max_speed, cfg.max_speed, num_velocities)\n",
    "    P, V = np.meshgrid(positions, velocities)\n",
    "    Z = np.zeros_like(P)\n",
    "\n",
    "    for i in range(P.shape[0]):\n",
    "        for j in range(P.shape[1]):\n",
    "            s = np.array([P[i, j], V[i, j]], dtype=np.float32)\n",
    "            q_vals = value_function.q_all(s)\n",
    "            Z[i, j] = -np.max(q_vals)\n",
    "\n",
    "    ax.plot_surface(P, V, Z, rstride=1, cstride=1, linewidth=0, antialiased=True)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Position\")\n",
    "    ax.set_ylabel(\"Velocity\")\n",
    "    ax.set_zlabel(\"Cost-to-go\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run experiments (analogues of Figures 10.1–10.4)\n",
    "# ============================================================\n",
    "output_dir = \"generated_pictures\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tilings = 8\n",
    "\n",
    "# ---------- Figure 1: evolving cost-to-go ----------\n",
    "episodes_total = 9000\n",
    "snapshots = [0, 99, episodes_total - 1]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "axes = [fig.add_subplot(1, len(snapshots), i + 1, projection=\"3d\") for i in range(len(snapshots))]\n",
    "\n",
    "step_size_for_surface = 0.3  # α × tilings\n",
    "vf_surface = ActionValueFunction(step_size_scaled=step_size_for_surface, num_tilings=tilings)\n",
    "\n",
    "for ep in tqdm(range(episodes_total), desc=\"Figure 1 training\"):\n",
    "    run_n_step_sarsa_episode(vf_surface, n=4, epsilon=0.0)\n",
    "    if ep in snapshots:\n",
    "        idx = snapshots.index(ep)\n",
    "        plot_cost_surface(vf_surface, axes[idx], f\"Episode {ep + 1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"figure_10_1.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ---------- Figure 2: effect of step-size ----------\n",
    "runs = 10\n",
    "episodes = 500\n",
    "step_sizes_list = [0.1, 0.2, 0.5]  # these are α × tilings\n",
    "times = np.zeros((len(step_sizes_list), episodes))\n",
    "\n",
    "for run in range(runs):\n",
    "    vfs = [ActionValueFunction(step_size_scaled=s, num_tilings=tilings) for s in step_sizes_list]\n",
    "    for idx, vf in enumerate(vfs):\n",
    "        for ep in tqdm(range(episodes), desc=f\"Figure 2 run {run+1}, α×m={step_sizes_list[idx]}\", leave=False):\n",
    "            steps_taken = run_n_step_sarsa_episode(vf, n=4, epsilon=0.0)\n",
    "            times[idx, ep] += steps_taken\n",
    "\n",
    "times /= runs\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, s in enumerate(step_sizes_list):\n",
    "    plt.plot(times[i], label=rf\"$\\alpha = {s}/{tilings}$\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Steps per episode (log scale, averaged over {runs} runs)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"figure_10_2.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ---------- Figure 3: n = 1 vs n = 8 ----------\n",
    "runs = 10\n",
    "episodes = 500\n",
    "step_sizes_n = [0.5, 0.3]  # α × tilings for n=1 and n=8\n",
    "n_values = [1, 8]\n",
    "times_n = np.zeros((len(step_sizes_n), episodes))\n",
    "\n",
    "for run in range(runs):\n",
    "    vfs = [ActionValueFunction(step_size_scaled=step_sizes_n[i], num_tilings=tilings)\n",
    "           for i in range(len(step_sizes_n))]\n",
    "    for idx, vf in enumerate(vfs):\n",
    "        n_current = n_values[idx]\n",
    "        for ep in tqdm(range(episodes), desc=f\"Figure 3 run {run+1}, n={n_current}\", leave=False):\n",
    "            steps_taken = run_n_step_sarsa_episode(vf, n=n_current, epsilon=0.0)\n",
    "            times_n[idx, ep] += steps_taken\n",
    "\n",
    "times_n /= runs\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, n_val in enumerate(n_values):\n",
    "    plt.plot(times_n[i], label=f\"n = {n_val:.1f}\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Steps per episode (log scale, averaged over {runs} runs)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"figure_10_3.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ---------- Figure 4: effect of α × tilings and n ----------\n",
    "runs = 5\n",
    "episodes = 50\n",
    "max_steps = ENV.cfg.max_steps\n",
    "\n",
    "alpha_scaled_values = np.arange(0.25, 1.75, 0.25)   # α × tilings\n",
    "n_grid = np.power(2, np.arange(0, 5))               # 1, 2, 4, 8, 16\n",
    "mean_steps = np.zeros((len(n_grid), len(alpha_scaled_values)))\n",
    "\n",
    "for run in range(runs):\n",
    "    for n_idx, n_val in enumerate(n_grid):\n",
    "        for a_idx, alpha_scaled in enumerate(alpha_scaled_values):\n",
    "            # for some (n, α) pairs the algorithm diverges – treat as max cost\n",
    "            if (n_val == 8 and alpha_scaled > 1.0) or (n_val == 16 and alpha_scaled > 0.75):\n",
    "                mean_steps[n_idx, a_idx] += max_steps * episodes\n",
    "                continue\n",
    "\n",
    "            vf = ActionValueFunction(step_size_scaled=alpha_scaled, num_tilings=tilings)\n",
    "            total_steps = 0\n",
    "            for ep in tqdm(range(episodes), desc=f\"Figure 4 run {run+1}, n={n_val}, α×m={alpha_scaled}\", leave=False):\n",
    "                total_steps += run_n_step_sarsa_episode(vf, n=n_val, epsilon=0.0)\n",
    "\n",
    "            mean_steps[n_idx, a_idx] += total_steps\n",
    "\n",
    "# average over runs and episodes\n",
    "mean_steps /= (runs * episodes)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, n_val in enumerate(n_grid):\n",
    "    plt.plot(alpha_scaled_values, mean_steps[i], label=f\"n = {n_val}\")\n",
    "plt.xlabel(r\"$\\alpha \\times$ number of tilings (8)\")\n",
    "plt.ylabel(f\"Steps per episode (avg over first {episodes} episodes and {runs} runs)\")\n",
    "plt.ylim([220, max_steps])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"figure_10_4.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"All figures saved into: {os.path.abspath(output_dir)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
